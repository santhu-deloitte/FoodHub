from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, when

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("CSV Transformation") \
    .getOrCreate()


# Read the CSV file
input_file = "foodhub_order.csv"  # Replace with your CSV file path
df = spark.read.csv(input_file, header=True, inferSchema=True)
#print(df.show(5))


# Perform transformations
# Example 1: Select specific columns
selected_df = df.select("restaurant_name", "cost_of_the_order")
print(selected_df.show(5))

# Example 2: Filter rows based on a condition
filtered_df = selected_df.filter(col("column1") > 10)


# Example 3: Add a new column
transformed_df = filtered_df.withColumn("new_column", col("column2") * 2)

# Show the transformed data
transformed_df.show()


# Write the transformed data to a new CSV file
output_file = "path/to/your/output.csv"  # Replace with your desired output file path
transformed_df.write.csv(output_file, header=True)

'''
# Group by restaurant name and count the number of orders
top_restaurants = df.groupBy("restaurant_name").agg(count("restaurant_name").alias("order_count"))

# Sort the restaurants by the number of orders in descending order
top_restaurants_sorted = top_restaurants.orderBy(col("order_count").desc())

# Display the top five restaurants
print("Top Five Restaurants in terms of the number of orders received:")
top_restaurants_sorted.show(5)

# Filter for weekend orders
weekend_orders = df.filter(col("day_of_the_week") == "Weekend")

# Group by cuisine type and count the number of orders
popular_cuisine = weekend_orders.groupBy("cuisine_type").agg(count("cuisine_type").alias("order_count"))

# Sort by the number of orders in descending order
popular_cuisine_sorted = popular_cuisine.orderBy(col("order_count").desc())

# Display the most popular cuisine on weekends
print("The most popular cuisine on weekends is:")
popular_cuisine_sorted.show(1)

# Group by restaurant name and calculate total rating count and average rating
restaurant_ratings = df.groupBy("restaurant_name").agg(
    count("rating").alias("rating_count"),
    avg("rating").alias("average_rating")
)

# Filter restaurants with rating count > 50 and average rating > 4
filtered_restaurants = restaurant_ratings.filter(
    (col("rating_count") > 50) & (col("average_rating") > 4)
)

# Display the list of restaurants
print("Restaurants with a rating count of more than 50 and an average rating greater than 4:")
filtered_restaurants.select("restaurant_name").show()'''

# Calculate the revenue for each order based on the conditions
df = df.withColumn(
    "company_revenue",
    when(col("cost_of_the_order") > 20, col("cost_of_the_order") * 0.25)
    .when(col("cost_of_the_order") > 5, col("cost_of_the_order") * 0.15)
    .otherwise(0)
)

# Calculate the total revenue
total_revenue = df.select(sum("company_revenue").alias("total_revenue")).collect()[0]["total_revenue"]

# Display the total revenue
print(f"The net revenue generated by the company across all orders is: ${total_revenue:.2f}")

# Stop the SparkSession
spark.stop()



